{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc81195d",
   "metadata": {},
   "source": [
    "# SQLSensei Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df016d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk, interleave_datasets\n",
    "from transformers import AutoTokenizer, TrainingArguments, GenerationConfig, Trainer, AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import warnings\n",
    "from google.colab import userdata\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee868c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dependencies automatmatically update\n",
    "# Future warnings can be safely ignored\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hugging face token (Running on Colab)\n",
    "userdata.get('huggingface')\n",
    "\n",
    "# Manually input hugging face token (Running Locally)\n",
    "hugging_face_token = 'token'\n",
    "\n",
    "# This notebook also requires a wandb API token to be entered when prompted\n",
    "\n",
    "# Choose the model\n",
    "model='t5-small'\n",
    "\n",
    "# Load the model's tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# Load the pre-trained model\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(model, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a073408",
   "metadata": {},
   "source": [
    "# Load Datasets from Hugging Face\n",
    "\n",
    "Each dataset is split into training and testing, and irrelevant columns are removed from each set. Then, they are merged into a single dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693aef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each dataset\n",
    "text_train = load_dataset(\"Clinton/Text-to-sql-v1\", split='train[:80%]')\n",
    "text_test = load_dataset(\"Clinton/Text-to-sql-v1\", split='train[-20%:-10%]')\n",
    "text_validation = load_dataset(\"Clinton/Text-to-sql-v1\", split='train[-10%:]')\n",
    "\n",
    "context_train = load_dataset(\"b-mc2/sql-create-context\", split='train[:80%]')\n",
    "context_test = load_dataset(\"b-mc2/sql-create-context\", split='train[-20%:-10%]')\n",
    "context_validation = load_dataset(\"b-mc2/sql-create-context\", split='train[-10%:]')\n",
    "\n",
    "know_train = load_dataset(\"knowrohit07/know_sql\", split='validation[:80%]')\n",
    "know_test = load_dataset(\"knowrohit07/know_sql\", split='validation[-20%:-10%]')\n",
    "know_validation = load_dataset(\"knowrohit07/know_sql\", split='validation[-10%:]')\n",
    "\n",
    "# Preprocess text-to-sql dataset\n",
    "text_train = text_train.remove_columns(['source', 'text'])\n",
    "text_train = text_train.rename_columns({'instruction': 'question', 'input': 'context', 'response': 'answer'})\n",
    "text_test = text_test.remove_columns(['source', 'text'])\n",
    "text_test = text_test.rename_columns({'instruction': 'question', 'input': 'context', 'response': 'answer'})\n",
    "text_validation = text_validation.remove_columns(['source', 'text'])\n",
    "text_validation = text_validation.rename_columns({'instruction': 'question', 'input': 'context', 'response': 'answer'})\n",
    "\n",
    "# Merge dataset\n",
    "merged_dataset = DatasetDict({ 'train': interleave_datasets([context_train, text_train, know_train]),\n",
    "                            'test': interleave_datasets([context_test, text_test, know_test]),\n",
    "                            'validation': interleave_datasets([context_validation, text_validation, know_validation])})\n",
    "\n",
    "# Save merged dataset\n",
    "merged_dataset.save_to_disk(\"merged_dataset\")\n",
    "\n",
    "# Load merged dataset\n",
    "merged_dataset = load_from_disk(\"merged_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9983ac",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Here, the data are cleaned and formatted with an NLP function. Then, the data are tokenized and loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ede954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NLP function\n",
    "def nlp_fn(sql_input):\n",
    "    \"\"\"This function takes a SQL input and performs NLP on it.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    sql_input: dict; dictionary containing the SQL context and question\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    SQL input, tokenized and ready for model usage\n",
    "    \"\"\"\n",
    "\n",
    "    # Pack context and question together\n",
    "    data_zip = zip(sql_input['context'], sql_input['question'])\n",
    "\n",
    "    # Generate appropriate prompt for each query\n",
    "    prompt = [\"Tables:\\n\" + context + \"\\n\\nQuestion:\\n\" + question + \"\\n\\nAnswer:\\n\" for context, question in data_zip]\n",
    "\n",
    "    # Tokenize the IDs\n",
    "    sql_input['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Tokenize the labels\n",
    "    sql_input['labels'] = tokenizer(sql_input['answer'], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    return sql_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the NLP function on the whole dataset in batches for each split\n",
    "tokenized_datasets = merged_dataset.map(nlp_fn, batched=True)\n",
    "\n",
    "# Remove non-tokenized columns\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['question', 'context', 'answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16083caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenized datasets\n",
    "tokenized_datasets.save_to_disk(\"tokenized_datasets\")\n",
    "\n",
    "# Load tokenized datasets\n",
    "tokenized_datasets = load_from_disk(\"tokenized_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29142a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify dataset dimensions\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Testing: {tokenized_datasets['test'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee2d216",
   "metadata": {},
   "source": [
    "# Fine-Tuning\n",
    "\n",
    "### Hyperparameters:\n",
    "Learning Rate: $5$ x $10^{-3}$\n",
    "\n",
    "Warmup Steps (Stabilization): $100$\n",
    "\n",
    "Epochs: $2$\n",
    "\n",
    "Training/Evaluation Batch Size: $16$\n",
    "\n",
    "Weight Decay (Regularization): $0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2bd52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a deployable copy of the model already exists\n",
    "if os.path.exists(model_path):\n",
    "    # Load the model if it exists\n",
    "    finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"SQLSensei\", token=hugging_face_token)\n",
    "    train = False\n",
    "else:\n",
    "    # Prepare the model for training otherwise\n",
    "    train = True\n",
    "    finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"SQLSensei\", torch_dtype=torch.bfloat16)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188aa61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track training time\n",
    "%%time\n",
    "\n",
    "if train:\n",
    "\n",
    "    # Training arguments tuned to NVIDIA GeForce GTX 1060 limitations\n",
    "    # Sets unique training directory on Hugging Face dashboard\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./sql-training-{str(int(time.time()))}',\n",
    "        learning_rate=5e-3,\n",
    "        warmup_steps=100,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=35,\n",
    "        eval_strategy='epoch'\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=finetuned_model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['validation'],\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the fine tuned model\n",
    "    finetuned_model.save_pretrained(\"SQLSensei\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine tuned model\n",
    "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"SQLSensei\", token=hugging_face_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c0de6",
   "metadata": {},
   "source": [
    "# Testing SQLSensei\n",
    "\n",
    "Uses zero shot inference, SQLSensei can now be tested on some specific examples of the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb767ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on the 5th query of the dataset\n",
    "query_index = 5\n",
    "\n",
    "# Set the prompt format to input to the model\n",
    "prompt = f\"\"\"\n",
    "\n",
    "You are a powerful SQL AI capable of generating accurate SQL queries.\n",
    "\n",
    "Generate an answer query based on the given information.\n",
    "\n",
    "Tables:\n",
    "{dataset['test'][query_index]['context']}\n",
    "\n",
    "Question:\n",
    "{dataset['test'][query_index]['question']}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the prompt and set as input\n",
    "inputs = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# Run the model on the input and store the output\n",
    "output = tokenizer.decode(\n",
    "    finetuned_model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=200,\n",
    "    )[0],\n",
    "    skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6b77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison results\n",
    "print(f\"Input Prompt:\\n{prompt}\")\n",
    "print()\n",
    "print(f\"Target Answer:\\n{dataset['test'][query_index]['answer']}\\n\")\n",
    "print()\n",
    "print(f\"SQLSensei's Answer:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e6dc7",
   "metadata": {},
   "source": [
    "# Evaluate using ROUGE Metric\n",
    "Now that SQLSensei has proven its ability to generate at least one correct query, it can be evaluated on a small subset of the data to obtain the ROGUE metric for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only 20 references are used, for time convienience\n",
    "# Select a subset of 20 random queries\n",
    "start = random.randint(0, 100)\n",
    "\n",
    "end = start + 20\n",
    "\n",
    "questions = dataset['test'][start:end]['question']\n",
    "context = dataset['test'][start:end]['context']\n",
    "target_answers = dataset['test'][start:end]['answer']\n",
    "\n",
    "# Initiate empty lists to store outputs\n",
    "base_model_output = []\n",
    "finetuned_model_output = []\n",
    "\n",
    "# Run on every query in the subset\n",
    "for i, question in enumerate(questions):\n",
    "\n",
    "    # Define prompt for each query and context\n",
    "    prompt = f\"\"\"Tables:\n",
    "    {context[i]}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the input\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Extract the labeled target output\n",
    "    human_baseline_text_output = target_answers[i]\n",
    "\n",
    "    # Extract tokenized base model and SQLSensei output\n",
    "    base_model_outputs = base_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=300))\n",
    "    finetuned_model_outputs = finetuned_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=300))\n",
    "\n",
    "    # Extract base model and SQLSensei output as readable text\n",
    "    base_model_text_output = tokenizer.decode(base_model_outputs[0], skip_special_tokens=True)\n",
    "    finetuned_model_text_output = tokenizer.decode(finetuned_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Append base model and SQLSensei readable text output to list\n",
    "    base_model_output.append(base_model_text_output)\n",
    "    finetuned_model_output.append(finetuned_model_text_output)\n",
    "\n",
    "# Zip into tuple, convert to list\n",
    "zipped_outputs = list(zip(target_answers, base_model_output, finetuned_model_output))\n",
    "\n",
    "# Convert list to pandas data frame (can be used for visualization/storage)\n",
    "output_df = pd.DataFrame(zipped_outputs, columns = ['target_answers', 'base_model_output', 'finetuned_model_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d833044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROGUE accuracy score for both models\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "base_model_results = rouge.compute(\n",
    "    predictions=base_model_output,\n",
    "    references=target_answers[0:len(base_model_output)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "print('Base T5 Model Metrics:')\n",
    "print(base_model_results)\n",
    "\n",
    "finetuned_model_results = rouge.compute(\n",
    "    predictions=finetuned_model_output,\n",
    "    references=target_answers[0:len(finetuned_model_output)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('SQLSensei Model Metrics:')\n",
    "print(finetuned_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491101b4",
   "metadata": {},
   "source": [
    "SQLSensei Testing Accuracy: 90%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
